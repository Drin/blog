# Paper Review: Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing

## Review

#### What's new
* Resilient Distributed Dataset (RDD) abstract data type for data distributed over multiple machines
* Materialization of RDDs after *actions* not *transformations*
* Method for persisting dataset to memory or disk at a given point (after any number of transformations or actions)

#### Summary
Resilient Distributed Datasets (RDDs) are an abstraction for working with a set of data in a distributed system. The idea being that you can invoke functions on the dataset as a set (much like queries are executed over database data), and the system can optimize function ordering, scheduling, intermediate state, and fault-tolerance. Some functions are lazy by definition (transformations), while others are eager (actions). Knowing the difference between transformations and actions helps the developer reason about the effects of data shuffles, and the *persist* method helps the developer explicitly tell the system what data should be cached and where to cache it.


#### Thoughts/Comments
1. Being able to mark an RDD as cache-able at a given state is very powerful since it allows the developer to re-use or re-compute from a given state efficiently, but I feel like it should be observable with a sophisticated type system to know when computations use the same source data. I would think this should be heavily related to register allocation and graph coloring, but perhaps because this is an NP-complete problem, it a reasonable trade-off to let the developer specify it.

2. The use of a lineage graph for fault-tolerance seems like a combination of logging (for durability and recovery) and task graph analysis (for scheduling and parallelization). Thinking to MapReduce, mappers know what partition of the input they should work on, so as long as the mapper is idempotent and metadata for the mapper input is tracked, then fault-tolerance for mappers is relatively straightforward. Lineage graphs are essentially the same thing, but the "map function" is the sequence of transformations and actions on a partition of the RDD. I suppose the authors had the thought, "why do we need to specify a map function and a reduce function, if we can frame the whole distributed computation as a sequence of functions to be applied everywhere". In this way, RDDs seem like a naturally higher level construct for distributed computation compared to MapReduce.
