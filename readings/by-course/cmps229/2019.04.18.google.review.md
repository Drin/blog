> What is the problem the authors are trying to solve?

The authors are interested in a fault-tolerant distributed file system
that is very fast on cheap, commodity hardware. Additionally, the authors
would like the file system to be fast for Google's workloads, which are
distinct from typical workload assumptions made by other file systems.

> What other approaches or solutions existed at the time that this
> work was done?

The authors mention a large range of existing work that they build on or
have similarities to. About 20 years prior, CMU had been building the
Andrew File System (AFS) to support 5000-10000 workstations. But GFS is
claimed to be more similar to xFS, which is based on RAID and Zebra, and
to Swift, which tries to hide latency by using high-speed interconnections
to many slow storage devices used in parallel.

Harp was a replicated Unix file system first described in 1991 that used
primary-copy replication for reliability. The authors mention they, "could
adapt a primary-copy scheme like the one in Harp... [for] high availability
and stronger consistency guaranees.

Lustre is a parallel file system which the authors mention had similar issues
as GFS when serving so many clients.

<!-- TODO -->
NASD (Network attached secure disk)
River (distributed memory-based queues)


> What was wrong with the other approaches or solutions?


> What is the authors' approach or solution?


> Why is it better than the other approaches or solutions?


> How does it perform?


> Why is this work important?


> 3+ comments/questions

1. 

2. 
   
3. 
