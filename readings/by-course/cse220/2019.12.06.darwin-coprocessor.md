# CSE 220 Paper Review <br/> Darwin - A Genomics co-processor for long read assembly


## Summary

This paper describes a co-processor that optimizes DNA assembly for long reads (average string length
of 10000 characters) where previous work optimizes for short reads (average string length of 100
characters). Background of the workload is given: assembly is typically a dynamic programming algorithm,
but an optimization finds exact matches of a short length, and extends string matches on either side of
the matched substring (seed) of a read (query string) compared to the reference assembly (reference
string). In addition to handling reference-based assembly (match DNA fragments to a pre-assembled genome),
the co-processor is also able to handle other, similar workloads--homology search, de-novo assembly (assembly
of a genome without a reference), and others. Then, the hardware design is described: the algorithm for the
filtering step (matching short substrings), D-SOFT, is addressed using a network-on-chip (NoC) with a butterfly
design; and the alignment step, GACT, is addressed using a systolic array of processing elements (PE), where
each PE has a 2kb SRAM for traceback (optimal solution in dynamic programming matrix. The extend step uses
wavefront parallelism to fill the dynamic programming matrix efficiently. The filtering step has copies of
query strings, reference strings, and all parameters in each DRAM channel in order to maximize the utilization
of each DRAM.

There are a few different aspects of evaluation included in the paper. One that I find interesting is the
area and power breakdown of Darwin components, which also mentions that the critical path of Darwin is 1.18 ns
in the table caption. There is also a table of error rates for 3 different sequencing technologies (pacbio,
nanopore 2D, nanopore 1D) and how well they were aligned for two parameters: T (tiles, or entries in the
dynamic programming table) and O (I'm still not sure what this is). For 1000 length reads up to 10000, GACT
(Darwin) performs orders of magnitude faster than GACT (software). For reads with 1000 bases, the Darwin
implementation achieves 4.3 million alignments per second compared to 100 alignments per second in software.
For longer reads, this throughput decreases, but the speedup increases (throughput of the software implementation
decays more than throughput of the Darwin implementation). Even in Table 4, where Darwin is compared to baseline,
there are small improvements in sensitivity and precision (small meaning less than 5%, but every increase is
very valuable) but large improvements in throughput.


## What are the paper's strengths?

Oddly enough, although I knew the rough idea of filtering algorithms such as BLAST, this paper illustrated
the algorithms and parameters in diagrams that were particularly effective. Likely these diagrams are more
effective since I already had knowledge of these algorithms, but I think the time taken to describe the
algorithms is essential because this is a point of unexpected difficulty when trying to do projects with
scientists.


## What are the paper's weaknesses?

Near the end of the introduction, Darwin is mentioned to be a hardware-acceleration framework. Then, in section
5, Darwin is defined as a co-processor for genomic sequence alignment that combines two hardware-accelerated
algorithms. This is certainly a minor point, but I read this paper looking for hints of how to extract framework
insights only to find that the "framework" is perhaps how they hardware-accelerated the algorithms? I'm still
not sure, to be honest.

I'm not really sure what weaknesses this paper has.

## Suggestions to improve paper

None, really.

## Impact and more recent work

The authors primarily talk about extending this work to metagenomics and other problems that require sequence
alignments. These problems can be very helpful for many areas of genomics, but I suppose co-processors necessarily
have a narrow scope--it's an accelerator for a type of workload. I think there are other areas that could potentially
benefit by being transformed into a string matching problem. I watched part of a talk that talked about a
quasi-polynomial algorithm for graph isomorphism that uses the idea of string matching as a useful reduction
in the proof.

I think the impact of this paper can be huge. Sequence alignment is an expensive step that is at the front of
nearly every bioinformatics pipeline. This can drastically reduce the end-to-end time of a pipeline that can help
with identifying poential cancer vaccines or for other drug and immunological applications. It would have been
cool to see how noticeable an accelerator like this is in a pipeline that does sequence alignment and how much
impact it has on the whole pipeline, just for a concrete frame of reference.

## Citation information

    @inproceedings{conf:darwin-coprocessor,
        author    = {Yatish Turakhia and Gill Bejerano and William J. Dally},
        title     = {Darwin - A Genomics Co-processor Provides up to 15000x acceleration on long
                     read assembly},
        series    = {ASPLOS '18},
        year      = {2018},
        location  = {Williamsburg, VA, USA},
        pages     = {199--213},
        numpages  = {15},
        url       = {http://doi.acm.org/10.1145/3173162.3173193},
        doi       = {10.1145/3173162.3173193},
        acmid     = {3173193},
        publisher = {ACM},
    }
