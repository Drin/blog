# Reading Summary 2019.05.24

&middot; by Aldrin Montana

## Amazon Aurora: On Avoiding Distributed Consensus for I/Os, Commits, and Membership Changes

#### Overall Evaluation

Amazon Aurora is a relational database offered by AWS that provides a relational front-end and
kernel (MySQL or PostgreSQL) on a highly available, causally consistent (my interpretation)
storage system (S3). In order to avoid network amplification and improve throughput, Aurora
does not use consensus for transaction commits or acknowledgements. A write is viewed as a
redo log record (traditional) and is replicated to storage nodes asynchronously, which ack
asynchronously once it is made durable. Then, separately redo log records are sorted
and propagated to other storage nodes via gossip. The sorted redo log records form causal
history, and the largest LSN for which all log records are contiguous (no holes) determines
when segments are complete (what differentiates segments is probably based on content?).

Then, contiguous LSNs acknowledged at various granularities maintain system consistency.
Segment-complete (SCL) determines consistency at the segment level, protection group-complete
(PGCL) determines consistency at the protection group level--which is the quorum group level,
and volume-complete (VCL) determines consistency across protection groups and for the database
instance. Then, to commit a transaction, a database instance waits until the volume is completed,
including writes for the active transaction (VCL >= system commit number (SCN)).

#### Strong Points

1. The group commit approach seems very nice as it decouples the system progressing and acknowledging
writes from when a transaction commits, which I think makes transactions minimal overhead, assuming
that data writes are the main unit of useful work instead of transactions.

2. When taking Lindsey Kuper's class in the Fall, I seemed to have derived something similar as
what is mentioned in section 4.2 about quorum sets of unlike members. I think this is a very
awesome idea and can probably be carefully planned in such a way that quorum overlap usually
hits the fast nodes, and thus can provide information that is separately mentioned in 3.1 and 3.2.

3.

#### Weak Points

1. No evaluation

2. 

3.
  
#### Questions Raised

1. What is a protection group? If segments A1 to F1 are replicas of each other, and odd segments
are acknowledged at PG1 and even segments are acknowledged at PG2, are PGs separate storage nodes?
There are only 6 replicas of each storage node, and there's only 1 primary instance, so I am not
clear what a PG is.

2. Actually, if I understand protection groups correctly, they are a set of storage nodes, where
each storage node stores a segment and peer storage nodes store a replica of the segment. Then,
having 2 protection groups means that there are 12 storage nodes, split across the 2 protection
groups, sharing load and improving throughput. This seems powerful, but I wonder if it also enables
fast striping for large blobs or provides "just enough" availability.

#### Research Connections
